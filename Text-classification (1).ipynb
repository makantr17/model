{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "def load_doc(filename):\n",
    "# open the file as read only\n",
    "    file = open(filename, 'r' )\n",
    "# read all text\n",
    "    text = file.read()\n",
    "# close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "# split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "# prepare regex for char filtering\n",
    "    re_punc = re.compile( ' [%s] ' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "    tokens = [re_punc.sub( '' , w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "# filter out stop words\n",
    "    stop_words = set(stopwords.words( 'english' ))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "# filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "# load doc\n",
    "    doc = load_doc(filename)\n",
    "# clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "# update counts\n",
    "    vocab.update(tokens)\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "# walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "# skip any reviews in the test set\n",
    "        if filename.startswith( 'cv9' ):\n",
    "            continue\n",
    "# create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "# add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36053\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs( 'txt_sentoken/pos' , vocab)\n",
    "process_docs( 'txt_sentoken/neg' , vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 7974), ('one', 4939), ('movie', 4815), ('like', 3193), ('even', 2261), ('good', 2073), ('time', 2039), ('story', 1899), ('would', 1843), ('much', 1823), ('also', 1757), ('get', 1723), ('character', 1699), ('two', 1642), ('characters', 1618), ('first', 1586), ('see', 1553), ('way', 1515), ('well', 1477), ('make', 1418), ('really', 1400), ('little', 1347), ('films', 1338), ('life', 1329), ('plot', 1286), ('people', 1267), ('could', 1248), ('bad', 1246), ('scene', 1240), ('never', 1197), ('best', 1176), ('new', 1139), ('scenes', 1132), ('many', 1129), ('man', 1122), ('know', 1092), ('movies', 1027), ('great', 1011), ('another', 992), ('action', 980), ('love', 975), ('us', 967), ('go', 950), ('director', 947), ('something', 944), ('end', 943), ('still', 935), ('seems', 930), ('back', 921), ('made', 911)]\n",
      "13069\n"
     ]
    }
   ],
   "source": [
    "# # print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "# # keep tokens with a min occurrence\n",
    "min_occurane = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "def save_list(lines, filename):\n",
    "    data = ' \\n ' .join(lines)\n",
    "# open file\n",
    "    file = open(filename, 'w' )\n",
    "# write text\n",
    "    file.write(data)\n",
    "# close file\n",
    "    file.close()\n",
    "\n",
    "# # save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "# split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "# prepare regex for char filtering\n",
    "    re_punc = re.compile( ' [%s] ' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "    tokens = [re_punc.sub( '' , w) for w in tokens]\n",
    "# filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' ' .join(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "# walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "# skip any reviews in the test set\n",
    "        if is_train and filename.startswith( 'cv9' ):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith( 'cv9' ):\n",
    "            continue\n",
    "# create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "# load the doc\n",
    "        doc = load_doc(path)\n",
    "# clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "# add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    neg = process_docs( 'txt_sentoken/neg' , vocab, is_train)\n",
    "    pos = process_docs( 'txt_sentoken/pos' , vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(tokenizer, max_length, docs):\n",
    "# integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "# pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding= 'post' )\n",
    "    return padded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation= 'relu' ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation= 'relu'))\n",
    "    model.add(Dense(1, activation= 'sigmoid' ))\n",
    "# compile network\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "# summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file= 'model.png' , show_shapes=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Maximum length: 1180 \n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1180, 100)         1307000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1173, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 586, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18752)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                187530    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,520,173\n",
      "Trainable params: 1,520,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# # define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# print( ' Vocabulary size: %d ' % vocab_size)\n",
    "# # calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print( ' Maximum length: %d ' % max_length)\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "# define model\n",
    "model = define_model(vocab_size, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 56s - loss: 0.6912 - acc: 0.5222\n",
      "Epoch 2/10\n",
      " - 53s - loss: 0.6114 - acc: 0.6606\n",
      "Epoch 3/10\n",
      " - 55s - loss: 0.2233 - acc: 0.9100\n",
      "Epoch 4/10\n",
      " - 52s - loss: 0.0302 - acc: 0.9967\n",
      "Epoch 5/10\n",
      " - 53s - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 54s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 52s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 53s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 48s - loss: 8.6973e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 49s - loss: 6.9222e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f105a46dbe0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save( 'model.h5' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 87.500000 \n"
     ]
    }
   ],
   "source": [
    "# load all reviews\n",
    "from keras.models import load_model\n",
    "\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "# encode data\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "model = load_model( 'model.h5' )\n",
    "# _, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "# print( ' Train Accuracy: %f ' % (acc*100))\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print( ' Test Accuracy: %f ' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "# clean review\n",
    "    line = clean_doc(review, vocab)\n",
    "# encode and pad review\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "# predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "# retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Review: [ Everyone will enjoy this film. I love it, recommended! ]\n",
      "Sentiment: NEGATIVE (50.973%) \n",
      " Review: [ This is a bad movie. Do not watch it. It sucks. ]\n",
      "Sentiment: NEGATIVE (52.625%) \n"
     ]
    }
   ],
   "source": [
    "text = ' Everyone will enjoy this film. I love it, recommended! '\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print( ' Review: [%s]\\nSentiment: %s (%.3f%%) ' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = ' This is a bad movie. Do not watch it. It sucks. '\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print( ' Review: [%s]\\nSentiment: %s (%.3f%%) ' % (text, sentiment, percent*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
